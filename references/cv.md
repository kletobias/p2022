---
layout: cv
title: Tobias Klein's CV
---
# Tobias Klein


<div id="webaddress">
<a href="unveil.nuggets@gmail.com">unveil.nuggets@gmail.com</a>
| <a href="http://deep-learning-mastery.com">My Portfolio Website</a>
</div>


## Currently

Participating in Machine Learning competitions and expanding my toolkit
full-time.

### Specialized In Machine Learning

**Solving Machine Learning Problems using Python** and an iterative process,
that generally incorporates the following steps, possibly going back and forth
between the steps:

1. Define the problem
    - Is it a regression problem
    - Is it a classification problem
    - Is it other higher order problem type
2. Prepare Data
    - Data Preprocessing
    - Feature Selection
    - Feature Engineering
3. Models
    - Candidate Model Selection
    - Hyperparameter Optimization (optional: depending on model)
    - Model Evaluation/Interpretation
    - Finalize Model


## Language Proficiency

- German: Native
- English: Bilingual Proficiency
- Spanish: Elementary Proficiency

## Education

### Higher Education

`2009 - 2011` **Heinrich Heine University**, Düsseldorf<br>
- **Subject:** Law (Jura)<br>
- **Optional Information:**
    - **Courses Completed**
        - Zivilrecht: BGB AT, Schuldrecht AT & BT, Hausarbeit;
        - Strafrecht: STGB AT I-II, Hausarbeit im Strafrecht, Übung im Strafrecht;
        - Öffentliches Recht: Polizeirecht, Grundrechte, Allg. Verwaltungsrecht,
        - Verwaltungsprozessrecht; 1. Teil Kurs im Angloamerikanischen Rechtbr>

`2011 - 2016` **LMU/Freiburg University**, Munich/Freiburg i. Br.<br>
- **Subject:** Mathematics B.Sc.<br>
- **Optional Information:**
    - **Courses Completed**
        - Linear Algebra I-II | Grades: {'I': 1.7, 'II': 2.3}
        - Analysis I-III | Grades: {'I': 1.3, 'III': 1.7}
        - Stochastic | Grade: 4.0
        - Complex Analysis | Grade: 4.0
        - Futures And Options | Grade: 3.7
        - Exercise In Numerics Using **C** To Implement Methods From Linear Algebra.

`2016 - 2019` **Freiburg University**, Freiburg i. Br.<br>
- **Subject:** Economics Focused Business Administration (BWL Non-Profit &
    Public Management)<br>
- **Final Grade:** 1.6
- **Bachelor Thesis:** 
    - **Title:** *'Data Mining: Hyperparameter Optimization For Real Estate Prediction Models'*
    - **Written Using:** *Latex* & *Python*
    - **Pages:** 69
    - **Grade:** 1.0
    - **Abstract:** See Section 'Bachelor Thesis' 
    - <strong><a href="https://filehost1.s3.eu-central-1.amazonaws.com/kletobias_github_io/klein-2019-data_mining_hyperparameter_optimisation_for_real_estate_prediction_models.pdf">Link: Full Text (PDF)</a></strong><br><br>
- **Optional Information:**
    - **Table: Relevant Courses Completed & Grade**<br>

| **Course**                                                                                      | **Grade** |
|-------------------------------------------------------------------------------------------------|-----------|
| Bachelor Thesis<br>'Data Mining: Hyperparameter Optimization for Real Estate Prediction Models' | 1.0       |
| Business Intelligence                                                                           | 1.3       |
| Econometrics                                                                                    | 1.0       |
| Electives in Non-Profit Management                                                              | 2.3       |
| Foundations of Economic Policy                                                                  | 1.0       |
| Fundamentals of Public Management: Foundation of Public Management                              | 1.3       |
| Game Theory: Spieltheorie                                                                       | 1.0       |
| Health Care Management I: Gesundheitsmanagement I                                               | 2.3       |
| Human Resources and Organization: Human Resources and Organisation                              | 2.3       |
| Introduction to Information Systems                                                             | 1.0       |
| Law & Economics                                                                                 | 2.0       |
| Management and Theory of the Firm: Unternehmenstheorie                                          | 1.3       |
| Mathematics                                                                                     | 1.3       |
| Microeconomics I                                                                                | 1.3       |
| Microeconomics II: Mikroökonomik II                                                             | 1.7       |
| New Public Management                                                                           | 1.3       |
| Non-Profit Organizations                                                                        | 1.3       |
| Public Finance I                                                                                | 2.0       |
| Public Finance II                                                                               | 1.0       |
| Tax Management (Seminar)                                                                        | 1.3       |


### School

`1993 - 1994` **Lincoln Elementary School**, Kampala, Uganda

`1994 - 1995` **Kindergarten**, Freiburg i. Br.

`1995 - 1999` **Elementary School**, Zell a.H.

`1999 - 2006` **High School**, Freiburg i. Br.

`2006 - 2006` **Secondary School**, Whistler, Canada<br>
 - Full-time mountain bike junior development program with participation in
     races.

`2006 - 2007` **High School**, Freiburg i. Br.

`2007 - 2008` **High School**, Hamburg<br>
- Graduated with Abitur.


## Bachelor Thesis

### Data Mining: Hyperparameter Optimization For Real Estate Prediction Models
<strong>Abstract:</strong><br>
Combining a highly scalable and customisable process,
with very accurate prediction results using machine learning models, is what
this work proposes. The customisation is guided by what information the user
seeks to gain from the process. This makes the process applicable for a variety
of sectors, such as Banking & Finance, Marketing and urban development among
others. It evaluates the process of using self-acquired data from an online real
estate platform, gained from deploying a custom web scraping algorithm. This
data is then combined with several spatial features for predicting the base rent
for apartments on a validation dataset. The analysis and predictions are made
for rental apartment listings within the Hanseatic City of Hamburg. The spatial
features originate from sources other than that of the apartments data and have
to be adapted to it first, therefore. Predictions are made using state of the
art machine learning models, in the form of a **Lasso Regression** model and a
**XGBoost Regressor** model. The Hyperparameter Optimisation techniques **grid
search** and **random search** are compared, during the optimisation process.
The focus is on maximising prediction accuracy of the models. The best scores,
expressed in **RMSE**, are 190.68 for the **Lasso** and 115.39 for the **XGBoost
Regressor**. Differences in complexity and interpretability between the models
are discussed and associated with it, the strengths and weaknesses of the
respective model are pointed out.<br>
<strong><a href="https://filehost1.s3.eu-central-1.amazonaws.com/kletobias_github_io/klein-2019-data_mining_hyperparameter_optimisation_for_real_estate_prediction_models.pdf">Full Text (PDF)</a></strong><br>

<!-- WARNING: Maybe outdated articles section, generate new one with latest changes
    and no white space tags and categories!!! -->
## Articles

<!-- A list is also available [online](http://scholar.google.co.uk/citations?user=LTOTl0YAAAAJ) -->

<p><H4>Automation Using A Test Harness For Deep Learning: Part 1</H4><strong>Description:</strong> How to create and use a custom test harness, that automates many steps of the deep learning testing process. It lowers GPU idle time, lets one build more models, test more parameter combinations in less time. The fastai library for deep learning is used throughout this article.<br>                <strong>Tags:</strong>  ['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization', 'learning-rate', 'loss-function', 'stochastic-gradient-descent']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                2703 | <strong><a href="https://deep-learning-mastery.com/projects/1st_tm/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Automation Using A Test Harness For Deep Learning: Part 2</H4><strong>Description:</strong> This is Part 2 in the series, where we explore how the fastai deep learning library can be used to conduct structured empirical experiments on a novel and small dataset. The dataset consists of 850 images and an almost uniform distribution for the target labels. There are two labels in total, "male" and "female", that are assigned the gender of the model depicted in any of the images in the dataset.<br>                <strong>Tags:</strong>  ['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization','image-data']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                3060 | <strong><a href="https://deep-learning-mastery.com/projects/2nd_tm/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Advanced Geospatial Feature Creation</H4><strong>Description:</strong> Extensive cleaning and transformation of tabular data, in order to create geospatial features. Once processed, the results are clean GPS values as "Point" objects in decimal degrees format and names of all subway and suburban train stations within Hamburg, Germany.<br>                <strong>Tags:</strong>  ['data-cleaning', 'data-transformation', 'geospatial-feature-creation', 'regular-expression', 'shapely', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3777 | <strong><a href="https://deep-learning-mastery.com/projects/advanced-geospatial-feature-creation/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 1</H4><strong>Description:</strong> Data Preparation Series: Exploring Tabular Data With pandas: An Overview Of Available Tools In The pandas Library.<br>                <strong>Tags:</strong>  ['data-exploration', 'first-steps', 'introduction', 'pandas', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3443 | <strong><a href="https://deep-learning-mastery.com/projects/data_prep_1/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 2</H4><strong>Description:</strong> More efficient string data cleaning by using the pyjanitor module and method chaining.<br>                <strong>Tags:</strong>  ['data-cleaning', 'pandas', 'regular-expressions', 'string-manipulation', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3198 | <strong><a href="https://deep-learning-mastery.com/projects/data_prep_2/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 3</H4><strong>Description:</strong> Extensive cleaning and validation and creation of a valid GPS column from the records, by joining the longitude and latitude columns together using geometry object Point.<br>                <strong>Tags:</strong>  ['data-validation', 'dtype-timedelta64','geospatial-feature-engineering', 'pandas', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                2338 | <strong><a href="https://deep-learning-mastery.com/projects/data_prep_3/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 4</H4><strong>Description:</strong> Extensive data cleaning and validation using regular expressions. Showcase of how batch processing several columns of tabular data using pandas, pyjanitor and the re library can look like.<br>                <strong>Tags:</strong>  ['batch-processing', 'data-validation', 'pandas', 'regular-expressions', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                4192 | <strong><a href="https://deep-learning-mastery.com/projects/data_prep_4/">Full Article</a></strong><br>                <br><br></p>
<p><H4>MySQL Queries Using An AWS Redshift MySQL Database</H4><strong>Description:</strong> This article shows how one can use Python to import CSV files using Pandas into a MySQL database hosted on AWS using Redshift and how to formulate basic MySQL queries to get the data of interest.<br>                <strong>Tags:</strong>  ['mysql', 'AWS', 'pandas', 'tabular-data', 'query']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1982 | <strong><a href="https://deep-learning-mastery.com/projects/mysql-redshift-1/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Datacamp Concrete Regression Challenge</H4><strong>Description:</strong> This is the notebook I created to solve the datacamp concrete challenge within an hour. There are explanations for most of the code in this article and we look deeper into the workings of the Lasso regression model.<br>                <strong>Tags:</strong>  ['cross-validation', 'lasso-regression', 'math', 'multivariate-regression', 'regression-analysis']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1162 | <strong><a href="https://deep-learning-mastery.com/projects/regression60min_datacamp-concrete-challenge/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 1</H4><strong>Description:</strong> Preprocessing Data: Visualizing missing values on an 80 feature dataset. Strategies for filling missing values and using categorical embeddings.<br>                <strong>Tags:</strong>  ['categorical-embeddings', 'data-preprocessing', 'fastai', 'fill-strategies', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                5305 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-1/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 2</H4><strong>Description:</strong> Feature selection using model DecisionTreeRegressor from sklearn and the feature_importances_ method which is tested for deviations in its score.<br>                <strong>Tags:</strong>  ['decision-tree-regressor', 'feature-importance', 'feature-selection', 'sklearn', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1839 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-2/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 3</H4><strong>Description:</strong> RandomForestRegressor using feature_importances_ and out-of-bag error to asses model performance.<br>                <strong>Tags:</strong>  ['feature-importance', 'feature-selection', 'out-of-bag-error', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1351 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-3/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 4</H4><strong>Description:</strong> Interpretation Using Advanced Statistical Visualizations. Dendrogram, Spearman rank correlation, partial dependence plot, impact of independent variables for sample on predictions.<br>                <strong>Tags:</strong>  ['dendrogram', 'partial-dependence', 'spearman-rank-correlation', 'tabular-data', 'treeinterpreter']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1607 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-4/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 5</H4><strong>Description:</strong> Out-of-domain problem: What it is, why it is important, how to spot it and how to deal with it.<br>                <strong>Tags:</strong>  ['feature-importance', 'model-accuracy', 'out-of-domain-problem', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1030 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-5/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 6</H4><strong>Description:</strong> Kaggle Submission 1: Training RandomForestRegressor, fastai deep learning model using hyperparameter optimization techniques. Preprocessing of Kaggle test data.<br>                <strong>Tags:</strong>  ['data-preprocessing', 'fastai', 'hyperparameter-optimization', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                2647 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-6/">Full Article</a></strong><br>                <br><br></p>
<p><H4>Deep Dive Tabular Data Pt. 7</H4><strong>Description:</strong> Kaggle Submission 2: tabular_learner deep learning estimator optimized using manual hyperparameter optimization. XGBRegressor using RandomizedSearchCV and sampling from continuous parameter distributions.<br>                <strong>Tags:</strong>  ['hyperparameter-optimization', 'random-search', 'tabular-data', 'tabular_learner', 'xgboost-regressor']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                3459 | <strong><a href="https://deep-learning-mastery.com/projects/tabular_kaggle-7/">Full Article</a></strong><br>                <br><br></p>
<p><H4>The Math Behind "Stepping The Weights"</H4><strong>Description:</strong> In this article we highlight a key concept in the Stochastic Gradient Descent and explore the basics, that this optimization algorithm is derived of.<br>                <strong>Tags:</strong>  ['deep-learning', 'math', 'ordinary-least-squares', 'partial-derivate', 'stochastic-gradient-descent']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                1756 | <strong><a href="https://deep-learning-mastery.com/projects/theory_batch_gradient_descent/">Full Article</a></strong><br>                <br><br></p>

<!-- ### Journals -->

<!-- `1669` -->
<!-- Newton Sir I, De analysi per æquationes numero terminorum infinitas. --> 

<!-- `1669` -->
<!-- Lectiones opticæ. -->

<!-- etc. etc. etc. -->


<!-- ## Occupation -->

<!-- `1600` -->
<!-- __Royal Mint__, London -->

<!-- - Warden -->
<!-- - Minted coins -->

<!-- `1600` -->
<!-- __Lucasian professor of Mathematics__, Cambridge University -->



<!-- ### Footer

Last updated: January 2023 -->

